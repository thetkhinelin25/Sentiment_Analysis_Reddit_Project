{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "440890c8-d793-4aaf-a0e4-5ce449f83e6a",
   "metadata": {},
   "source": [
    "# Title: Data Collection and Sentiment Analysis\n",
    "\n",
    "## Working Procedure\n",
    "The program is structured to include separate user-defined functions for distinct tasks: data collection, CSV file saving, and sentiment analysis. These functions are orchestrated within the main function, which facilitates the workflow. The main function also uses a `while` loop to collect data and compute sentiment analysis scores in near real-time.\n",
    "\n",
    "### Data Collection Function\n",
    "This function utilizes the Reddit API to fetch data. It collects the following fields for each post:\n",
    "- **ID**: Unique identifier of the post.\n",
    "- **Title**: The title of the post.\n",
    "- **SelfText**: The body content of the post.\n",
    "- **Date**: The creation date of the post.\n",
    "\n",
    "The function ensures that only posts with non-missing `SelfText` data are collected, thereby avoiding empty or irrelevant entries.\n",
    "\n",
    "### Data Saving Function\n",
    "This function is responsible for handling and storing data. It:\n",
    "1. Combines newly collected data with previously saved data from the CSV file.\n",
    "2. Performs duplication checks to ensure no redundant entries are stored.\n",
    "\n",
    "### Sentiment Analysis Function\n",
    "This function applies sentiment analysis to text data using the **TextBlob** sentiment analysis model. It evaluates the sentiment of the text and generates sentiment scores, which are saved for further analysis.\n",
    "\n",
    "### Main Function\n",
    "The main function coordinates the workflow by invoking the individual functions. It contains a `while` loop to:\n",
    "1. Continuously collect new data at regular intervals.\n",
    "2. Save the collected data to the CSV file 1 for backup.\n",
    "3. Apply sentiment analysis to the collected data.\n",
    "4. Save the processed results to the CSV file 2, enabling near real-time analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eef98ee0-922b-4598-91e8-0e538f749b1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data from Reddit...\n",
      "Searching in subreddit: r/unitednations...\n",
      "r/unitednations: Done\n",
      "Searching in subreddit: r/climatechange...\n",
      "r/climatechange: Done\n",
      "Searching in subreddit: r/change...\n",
      "r/change: Done\n",
      "Searching in subreddit: r/heat...\n",
      "r/heat: Done\n",
      "Searching in subreddit: r/weather...\n",
      "r/weather: Done\n",
      "Searching in subreddit: r/environment...\n",
      "r/environment: Done\n",
      "Searching in subreddit: r/sustainability...\n",
      "r/sustainability: Done\n",
      "Searching in subreddit: r/renewableenergy...\n",
      "r/renewableenergy: Done\n",
      "Searching in subreddit: r/conservation...\n",
      "r/conservation: Done\n",
      "Appended 2 new posts. Total posts: 3973\n",
      "Waiting for the next interval...\n",
      "Fetching data from Reddit...\n",
      "Searching in subreddit: r/unitednations...\n",
      "r/unitednations: Done\n",
      "Searching in subreddit: r/climatechange...\n",
      "r/climatechange: Done\n",
      "Searching in subreddit: r/change...\n",
      "r/change: Done\n",
      "Searching in subreddit: r/heat...\n",
      "r/heat: Done\n",
      "Searching in subreddit: r/weather...\n",
      "r/weather: Done\n",
      "Searching in subreddit: r/environment...\n",
      "r/environment: Done\n",
      "Searching in subreddit: r/sustainability...\n",
      "r/sustainability: Done\n",
      "Searching in subreddit: r/renewableenergy...\n",
      "r/renewableenergy: Done\n",
      "Searching in subreddit: r/conservation...\n",
      "r/conservation: Done\n",
      "No new posts found.\n",
      "Waiting for the next interval...\n",
      "Fetching data from Reddit...\n",
      "Searching in subreddit: r/unitednations...\n",
      "r/unitednations: Done\n",
      "Searching in subreddit: r/climatechange...\n",
      "r/climatechange: Done\n",
      "Searching in subreddit: r/change...\n",
      "r/change: Done\n",
      "Searching in subreddit: r/heat...\n",
      "r/heat: Done\n",
      "Searching in subreddit: r/weather...\n",
      "r/weather: Done\n",
      "Searching in subreddit: r/environment...\n",
      "r/environment: Done\n",
      "Searching in subreddit: r/sustainability...\n",
      "r/sustainability: Done\n",
      "Searching in subreddit: r/renewableenergy...\n",
      "\n",
      "Script interrupted. Exiting...\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "from textblob import TextBlob  # Import TextBlob\n",
    "\n",
    "# Reddit API credentials\n",
    "client_id = 'wvFf0PPQo_1zth6SkDESUQ'\n",
    "client_secret = 'krMR9Hj-IILecymbX_kf4SdLHNv6Gg'\n",
    "user_agent = 'climate_sentiment_analyzer:v1.0 (by /u/Ok_Beginning1171)'\n",
    "\n",
    "# Authenticate with Reddit API using praw\n",
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    user_agent=user_agent\n",
    ")\n",
    "\n",
    "# Function to search Reddit by subreddit\n",
    "def search_reddit_by_subreddit(query, subreddit, limit=1000, collected_ids=set()):\n",
    "    posts = []\n",
    "    try:\n",
    "        subreddit_instance = reddit.subreddit(subreddit)\n",
    "        for submission in subreddit_instance.search(query, sort='new', limit=limit):\n",
    "            if submission.id not in collected_ids and submission.selftext.strip():\n",
    "                posts.append({\n",
    "                    'ID': submission.id,\n",
    "                    'Title': submission.title,\n",
    "                    'SelfText': submission.selftext,\n",
    "                    'Date': datetime.utcfromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data from subreddit r/{subreddit}: {e}\")\n",
    "    return posts\n",
    "\n",
    "# Function to save posts to CSV\n",
    "def save_to_csv(df_new_posts, filename):\n",
    "    df_updated = pd.DataFrame()\n",
    "    if os.path.exists(filename):\n",
    "        df_existing = pd.read_csv(filename)\n",
    "        df_updated = pd.concat([df_existing, df_new_posts]).drop_duplicates(subset=['ID']).reset_index(drop=True)\n",
    "    df_updated.to_csv(filename, index=False)\n",
    "    return len(df_new_posts), len(df_updated)\n",
    "\n",
    "# Define a function for sentiment scoring using TextBlob\n",
    "def analyze_sentiment(text):\n",
    "    if not text:\n",
    "        return 0.0  # Neutral score if no text\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity  # Polarity score: -1 (negative) to 1 (positive)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    queries = [\"climate change\", \"global warming\", \"greenhouse gases\", \"carbon emissions\", \"renewable energy\", \n",
    "                \"deforestation\", \"sea level rise\", \"extreme weather\", \"climate action\", \"fossil fuels\",\n",
    "                \"carbonfoot print\", \"united nations\"]\n",
    "\n",
    "    subreddits = [\"unitednations\", \"climatechange\", \"change\", \"heat\", \"weather\", \"environment\", \"sustainability\", \"renewableenergy\", \"conservation\"]\n",
    "    \n",
    "    filename = 'climate_change_posts_streaming.csv'\n",
    "    filename_sa = \"climate_change_sentiment_analysis_streaming.csv\"\n",
    "    collected_ids = set()\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            print(\"Fetching data from Reddit...\")\n",
    "            new_posts = []\n",
    "\n",
    "            for subreddit in subreddits:\n",
    "                print(f\"Searching in subreddit: r/{subreddit}...\")\n",
    "                for query in queries:\n",
    "                    if os.path.exists(filename):\n",
    "                        df_existing = pd.read_csv(filename)\n",
    "                        if 'ID' in df_existing.columns:\n",
    "                            collected_ids = set(df_existing['ID'].values)\n",
    "                \n",
    "                    subreddit_posts = search_reddit_by_subreddit(query, subreddit, collected_ids=collected_ids)\n",
    "                    new_posts.extend(subreddit_posts)\n",
    "                print(f\"r/{subreddit}: Done\")\n",
    "\n",
    "            df_new_posts = pd.DataFrame(new_posts).drop_duplicates(subset=['ID'])\n",
    "            \n",
    "            if df_new_posts.empty:\n",
    "                print(\"No new posts found.\")\n",
    "                \n",
    "            else:\n",
    "                # Save new posts to CSV\n",
    "                new_count, total_count = save_to_csv(df_new_posts, filename=filename)\n",
    "\n",
    "                # Perform sentiment analysis\n",
    "                df_new_posts['TitleSentimentScore'] = df_new_posts['Title'].apply(analyze_sentiment)\n",
    "                df_new_posts['SelfTextSentimentScore'] = df_new_posts['SelfText'].apply(analyze_sentiment)\n",
    "                \n",
    "                # Save sentiment analysis to a new CSV\n",
    "                header = not os.path.exists(filename_sa)\n",
    "                df_new_posts.to_csv(filename_sa, mode='a', index=False, header=header)\n",
    "\n",
    "                print(f\"Appended {new_count} new posts. Total posts: {total_count}\")\n",
    "\n",
    "            print(\"Waiting for the next interval...\")\n",
    "            time.sleep(5)\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nScript interrupted. Exiting...\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "            break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
